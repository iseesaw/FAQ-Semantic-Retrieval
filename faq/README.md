# FAQä¹‹åŸºäºBERTçš„è¯­ä¹‰å¬å›

## ä»‹ç»

å½“ç”¨æˆ·è¾“å…¥ query åï¼ŒFAQ çš„å¤„ç†æµç¨‹ä¸€èˆ¬ä¸º

- **é—®é¢˜ç†è§£**ï¼Œå¯¹ query è¿›è¡Œæ”¹å†™ä»¥åŠå‘é‡è¡¨ç¤º

- **å¬å›æ¨¡å—**ï¼Œåœ¨å¤§è§„æ¨¡é—®é¢˜é›†ä¸­è¿›è¡Œå€™é€‰é—®é¢˜å¬å›ï¼Œè·å¾— topk

- **æ’åºæ¨¡å—**ï¼Œå¯¹å¬å›åç»­é—®é¢˜é›†åˆè¿›è¡Œæ’åºï¼Œç„¶åè¿”å›æ¦‚ç‡æœ€å¤§åç»­é—®é¢˜å¯¹åº”çš„ç­”æ¡ˆ/å›å¤



æœ¬æ–‡ä¸»è¦è®¨è®º **å¬å›æ¨¡å—**ï¼Œå³è·å– topk åç›´æ¥è¿”å›ï¼Œä¸»è¦å†…å®¹åŒ…æ‹¬ï¼š

- **FAQ å¬å›æ–¹æ³•**ï¼ŒåŒ…æ‹¬**åŸºäºå…³é”®å­—çš„å€’æ’ç´¢å¼•**ï¼ˆä½¿ç”¨ ElasticSearch æˆ– Lucene æˆ– Whooshï¼‰ï¼›åŸºäº**å‘é‡æ£€ç´¢çš„è¯­ä¹‰å¬å›**ï¼Œä½¿ç”¨ BERT é¢„è®­ç»ƒæ¨¡å‹è¡¨ç¤ºé—®é¢˜ï¼Œå¹¶ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—è·ç¦»ï¼ˆä½¿ç”¨ sentence-transformers æˆ– transformers å®ç°ï¼‰

- **BERT å¾®è°ƒä¸è’¸é¦**ï¼Œæ„é€ è®­ç»ƒæ•°æ®é›†ï¼Œåœ¨ BERT æ¨¡å‹åŸºç¡€ä¸Š fine-tuneï¼Œè¿›è¡Œæ¨¡å‹è’¸é¦

- **FAQ çº¿ä¸ŠæœåŠ¡éƒ¨ç½²**ï¼Œä½¿ç”¨è¿›ç¨‹ç®¡ç†ä»¥åŠç¼“å­˜ç­‰æœºåˆ¶ï¼Œè¿›è¡Œå‹åŠ›æµ‹è¯•



## FAQ å¬å›æ–¹æ³•

> åŸºäºå…³é”®å­—å€’æ’ç´¢å¼•å’ŒåŸºäºå‘é‡çš„è¯­ä¹‰å¬å›å®ç°æ€è·¯

### å…³é”®å­—æ£€ç´¢ï¼ˆå€’æ’ç´¢å¼•ï¼‰

è®¡ç®—æ¯ä¸ªå…³é”®å­—åœ¨å„æ–‡æ¡£ä¸­çš„ $TF-IDF$ ä»¥åŠ $BM25$ å¾—åˆ†ï¼Œå¹¶å»ºç«‹å€’æ’ç´¢å¼•è¡¨

Python ç¬¬ä¸‰æ–¹åº“ [ElasticSearch](https://whoosh.readthedocs.io/en/latest/index.html)ï¼Œ[Lucene](https://lucene.apache.org/pylucene/)ï¼Œ[Whoosh](https://whoosh.readthedocs.io/en/latest/index.html)

#### TF-IDF

ç»™å®šæ–‡æ¡£é›†åˆ $\mathcal{D} = \{d_1, d_2, d_3, \cdots, d_n\}$ ï¼Œæ¯ä¸ªæ–‡æ¡£ä¸º $d_j = \{t_{1}, t_2, \cdots, t_m\}$

- $TF$ï¼ˆTerm Frequencyï¼‰

  - è¯é¢‘æ˜¯æŒ‡ä¸€ä¸ªç»™å®šè¯è¯­åœ¨è¯¥æ–‡æ¡£ä¸­å‡ºç°çš„é¢‘ç‡

  - è¯é¢‘æ˜¯è¯æ•°ï¼ˆTerm Countï¼‰çš„å½’ä¸€åŒ–ï¼Œé˜²æ­¢åå‘é•¿æ–‡æ¡£

  - å¯¹äºæŸä¸€ç‰¹å®šæ–‡æ¡£ $d_j$ é‡Œçš„è¯è¯­ $t_i$ ï¼Œå…¶åœ¨è¯¥æ–‡æ¡£ä¸­å‡ºç°æ¬¡æ•°ä¸º $n_{i,j}$ ï¼Œåˆ™æœ‰
    $$
    TF_{i,j} = \frac{n_{i,j}}{{\sum_k} n_{k,j}}
    $$

- $IDF$ï¼ˆInverse Document Frequencyï¼‰

  - é€†æ–‡æ¡£é¢‘ç‡ç”¨äºè¡¡é‡ä¸€ä¸ªè¯è¯­åœ¨æ‰€æœ‰æ–‡æ¡£ä¸­çš„é‡è¦æ€§

  - è¡¨ç¤ºè¶Šå¤šçš„æ–‡æ¡£ä¸­å‡ºç°è¯¥è¯è¯­ï¼Œ åˆ™å…¶é‡è¦æ€§è¶Šä½

  - è¯è¯­ $t_i$ çš„ $IDF$ å¾—åˆ†å¦‚ä¸‹ï¼Œå…¶ä¸­ $|\{j: t_i \in d_j\}|$ è¡¨ç¤ºåŒ…å«è¯è¯­ $t_i$ çš„æ–‡æ¡£ä¸ªæ•°
    $$
    IDF_{i} = \lg \frac{|D|}{|\{j: t_i \in d_j\}|}
    $$

- $TF-IDF$

  - è¯è¯­ $t_i$ å¯¹äºæ–‡æ¡£ $d_j$ çš„é‡è¦ç¨‹åº¦å¾—åˆ†ä¸º

  $$
  TF-IDF_{i,j} = TF_{i,j} \times IDF_{i}
  $$
  - ç‰¹å®šæ–‡æ¡£ä¸­çš„é«˜è¯é¢‘ï¼Œä»¥åŠè¯¥è¯è¯­åœ¨æ•´ä¸ªæ–‡æ¡£é›†åˆä¸­çš„ä½æ–‡ä»¶é¢‘ç‡ï¼Œå¯ä»¥äº§ç”Ÿé«˜æƒé‡çš„ TF-IDF å¾—åˆ†
  - TF-IDF å€¾å‘äºè¿‡æ»¤å¸¸è§çš„è¯è¯­ï¼Œä¿ç•™é‡è¦çš„è¯è¯­
  - è®¡ç®—ç®€å•ï¼Œä½†æ˜¯ç²¾åº¦ä¸é«˜ï¼Œä¸”æ²¡æœ‰ä½“ç°è¯è¯­çš„ä½ç½®ä¿¡æ¯



#### BM25

> [Okapi BM25](https://en.wikipedia.org/wiki/Okapi_BM25)

ç”¨äºæœç´¢ç›¸å…³æ€§è¯„ä¼°ï¼Œè®¾è¾“å…¥ $Query=\{q_1, q_2, \cdots, q_N\}$ ï¼Œ$BM25$ æ€æƒ³ä¸ºè®¡ç®— $q_i$ ä¸æ¯ä¸ªæ–‡æ¡£ $d$ çš„ç›¸å…³æ€§å¾—åˆ†ï¼Œç„¶åå°† $Query$ ä¸­æ‰€æœ‰ $q$ ç›¸å¯¹äº $d$ çš„ç›¸å…³æ€§å¾—åˆ†åŠ æƒæ±‚å’Œï¼Œä»è€Œå¾—åˆ° $Query$ ä¸ $d$ å¾—ç›¸å…³æ€§å¾—åˆ† $BM25(Query, d)$ 
$$
BM25(Query, d) = \sum_{i=1}^N w_i \times R(q_i, d)
$$

- å…¶ä¸­æ¯ä¸ªè¯è¯­çš„æƒé‡ $w_i$ å¯ä»¥ç”¨ $IDF$ è¡¨ç¤ºï¼ˆä¸ä¸Šè¿°æœ‰æ‰€ä¸åŒï¼Œè¿›è¡Œäº†å¹³æ»‘ç­‰å¤„ç†ï¼‰

  $$
  w_i = IDF(q_i) = \lg \frac{N - n_{q_i} + 0.5}{n_{q_i} + 0.5}
  $$

- è€Œè¯è¯­å¯¹äºæ–‡æ¡£çš„ç›¸å…³æ€§å¾—åˆ† $R(q_i, d)$ åˆ™ä¸º

  $$
  R(q_i, d) = \frac{f(q_i, d) \times (k_1 + 1)}{f(q_i, d) + k_1 \times (1 - b + b \times \frac{dl}{avgdl})}
  $$
  
  - å…¶ä¸­  $f(q_i, d)$ ä¸º $q_i$ åœ¨æ–‡æ¡£ $d$ ä¸­çš„è¯é¢‘
  - $k_1, b$ ä¸ºå¯è°ƒèŠ‚å‚æ•°ï¼Œé€šå¸¸è®¾ç½® $k_1 \in [1.2, 2.0] , b = 0.75$ ï¼Œ$b$ ç”¨äºè°ƒèŠ‚æ–‡æœ¬é•¿åº¦å¯¹ç›¸å…³æ€§çš„å½±å“
  
  - $dl,avgdl$ åˆ†åˆ«é—®æ–‡æ¡£ $d$ çš„é•¿åº¦å’Œæ–‡æ¡£é›† $D$ ä¸­æ‰€æœ‰æ–‡æ¡£çš„å¹³å‡é•¿åº¦



### å‘é‡æ£€ç´¢ï¼ˆè¯­ä¹‰å¬å›ï¼‰

ç¦»çº¿ä½¿ç”¨ BERT ç­‰é¢„è®­ç»ƒæ¨¡å‹å¯¹å€™é€‰é›†åˆä¸­æ‰€æœ‰é—®é¢˜è¿›è¡Œç¼–ç å¾—åˆ° corpus å‘é‡çŸ©é˜µï¼Œå½“ç”¨æˆ·è¾“å…¥ query æ—¶ï¼ŒåŒæ ·è¿›è¡Œç¼–ç å¾—åˆ°å‘é‡è¡¨ç¤ºï¼Œè®¡ç®— query å‘é‡å’Œ corpus å‘é‡çŸ©é˜µçš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œæ‰¾å› topk å€™é€‰é—®é¢˜

å‘é‡è¡¨ç¤ºå¯ä»¥ä½¿ç”¨å¦‚ä¸‹å‡ ç§æ–¹æ³•

| Python åº“                                                    | ç‰¹ç‚¹                                   |
| ------------------------------------------------------------ | -------------------------------------- |
| [bert-as-serivce](https://github.com/hanxiao/bert-as-service) | é«˜å¹¶å‘ï¼Œè‡ªå®šä¹‰ç¨‹åº¦ä½ï¼Œéš¾ä»¥æ‰©å±•å…¶ä»–æ¨¡å‹ |
| [Sentence-Transformers](https://www.sbert.net/index.html)    | æ”¯æŒå¤šç§æ¨¡å‹ï¼Œæ¥å£ç®€å•æ˜“ç”¨             |
| [transformers](https://github.com/huggingface/transformers/) | æ”¯æŒå¤šç§æ¨¡å‹ï¼Œè‡ªå®šä¹‰ç¨‹åº¦é«˜             |



#### Bert-as-service

 [bert-as-serivce](https://github.com/hanxiao/bert-as-service) æ˜¯åŸºäº Google å®˜æ–¹ä»£ç å®ç°çš„ BERT æœåŠ¡ï¼Œå¯ä»¥æœ¬åœ°æˆ–è€…HTTPè¯·æ±‚è¿›è¡Œå¥å­ç¼–ç 

å…¶é»˜è®¤çš„å¥å‘é‡æ„é€ æ–¹æ¡ˆæ˜¯å–å€’æ•°ç¬¬äºŒå±‚çš„éšçŠ¶æ€å€¼åœ¨tokenä¸Šçš„å‡å€¼ï¼Œå³é€‰ç”¨çš„å±‚æ•°æ˜¯å€’æ•°ç¬¬2å±‚ï¼Œæ± åŒ–ç­–ç•¥æ˜¯ REDUCE_MEAN

```python
# éœ€è¦è¿è¡ŒæœåŠ¡ bert-serving-start -model_dir /tmp/english_L-12_H-768_A-12/ -num_worker=4 
from bert_serving.client import BertClient

# åˆ›å»ºå®¢æˆ·ç«¯ç¤ºä¾‹å¹¶è¿›è¡Œç¼–ç 
bc = BertClient()
bc.encode(['First do it', 'then do it right', 'then do it better'])
```

é€šè¿‡ HTTP è¯·æ±‚

```python
# è¿è¡Œ http æœåŠ¡ bert-serving-start -model_dir=/YOUR_MODEL -http_port 8125
import requests
reply = requests.post('http://127.0.0.1:8125/encode',
                     data={
                        "id": 123,
                        "texts": ["hello world", "good day!"],
                        "is_tokenized": false
                    })
reply.json()
# {
#    "id": 123,
#    "results": [[768 float-list], [768 float-list]],
#    "status": 200
# }
```



#### Sentence-transformers

> å°½é‡ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬ï¼Œå¦åˆ™å¯èƒ½æŠ¥é”™
>
> ```bash
> torch                              1.6.0
> sentence-transformers              0.3.4
> transformers                       3.0.2
> ```

[Sentence-Transformers](https://www.sbert.net/index.html) æ˜¯ä¸€ä¸ªåŸºäº [Transformers](https://huggingface.co/transformers/) åº“çš„å¥å‘é‡è¡¨ç¤º Python æ¡†æ¶ï¼Œå†…ç½®è¯­ä¹‰æ£€ç´¢ä»¥åŠå¤šç§æ–‡æœ¬ï¼ˆå¯¹ï¼‰ç›¸ä¼¼åº¦æŸå¤±å‡½æ•°ï¼Œå¯ä»¥è¾ƒå¿«çš„å®ç°æ¨¡å‹å’Œè¯­ä¹‰æ£€ç´¢ï¼Œå¹¶æä¾›å¤šç§[é¢„è®­ç»ƒçš„æ¨¡å‹](https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/)ï¼ŒåŒ…æ‹¬ä¸­æ–‡åœ¨å†…çš„å¤šè¯­è¨€æ¨¡å‹

- [å†…ç½®é¢„è®­ç»ƒçš„æ¨¡å‹](https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/)

  ```python
  from sentence_transformers import SentenceTransformer, model

  # åŠ è½½å†…ç½®æ¨¡å‹ (https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/)
  model = SentenceTransformer('distilbert-multilingual-nli-stsb-quora-ranking')

  sentences = ['This framework generates embeddings for each input sentence',
      'Sentences are passed as a list of string.',
      'The quick brown fox jumps over the lazy dog.']

  # å¥å­ç¼–ç 
  embeddings = model.encode(sentences)
  ```

-  [HuggingFace çš„é¢„è®­ç»ƒæ¨¡å‹](https://huggingface.co/models) 

  > éœ€è¦æŒ‡å®šç‰¹å®šçš„ç±»ï¼Œæ¯”å¦‚ä½¿ç”¨ `bert-base-chinese` ï¼Œ[#issue75](https://github.com/UKPLab/sentence-transformers/issues/75#issuecomment-568717443)

  ```python
  from sentence_transformers import models

  model_name = 'bert-base-chinese'

  # ä½¿ç”¨ BERT ä½œä¸º encoder
  word_embedding_model = models.BERT(model_name)

  # ä½¿ç”¨ mean pooling è·å¾—å¥å‘é‡è¡¨ç¤º
  pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),
  pooling_mode_mean_tokens=True,
  pooling_mode_cls_token=False,
  pooling_mode_max_tokens=False)

  model = SentenceTransformer(modules=[word_embedding_model, pooling_model])
  ```



#### Transformers

[Transformers](https://huggingface.co/transformers/) æ”¯æŒå¤šç§é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¯æ‰©å±•æ€§å¼ºï¼Œé¢„è®­ç»ƒæ¨¡å‹é»˜è®¤è¾“å‡º `sequence_output` å’Œ `pool_output` åˆ†åˆ«ä¸ºå¥å­ä¸­æ¯ä¸ª token çš„å‘é‡è¡¨ç¤ºå’Œ [CLS] çš„å‘é‡è¡¨ç¤ºï¼ˆç›¸å…³è®ºæ–‡è¯æ˜ Average Embedding æ•ˆæœå¥½äº [CLS]ï¼‰

```python
from transformers import AutoTokenizer, AutoModel
import torch


# Mean Pooling - Take attention mask into account for correct averaging
def mean_pooling(model_output, attention_mask):
    # è·å¾—å¥å­ä¸­æ¯ä¸ª token çš„å‘é‡è¡¨ç¤º
    token_embeddings = model_output[0] 
    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)
    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
    return sum_embeddings / sum_mask


sentences = ['This framework generates embeddings for each input sentence',
             'Sentences are passed as a list of string.',
             'The quick brown fox jumps over the lazy dog.']

# åŠ è½½æ¨¡å‹æ–‡ä»¶
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModel.from_pretrained("bert-base-uncased")

# æ•°æ®é¢„å¤„ç†
encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=128, return_tensors='pt')

# æ¨¡å‹æ¨ç†
with torch.no_grad():
    model_output = model(**encoded_input)

# mean pooling è·å¾—å¥å­è¡¨ç¤º
sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])
```



## BERT å¾®è°ƒä¸è’¸é¦

åœ¨ä½¿ç”¨ transformers ç­‰åº“è·å–å¥å‘é‡è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨å·²æœ‰çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå¦‚ `bert-base-chinese` ç­‰

ä½†æ˜¯ä¸ºäº†åœ¨ç‰¹å®šé¢†åŸŸä¸Šè·å¾—æ›´å¥½çš„æ•ˆæœï¼Œå¯èƒ½éœ€è¦åœ¨ç›®æ ‡æ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒï¼ŒåŒæ—¶å¯ä»¥è¿›è¡Œæ¨¡å‹è’¸é¦ä¾¿äºéƒ¨ç½²åº”ç”¨

### æ•°æ®é›†

è¯­ä¹‰å¬å›æœ¬è´¨ä¸Šæ˜¯è®¡ç®—è¾“å…¥ query å’Œå€™é€‰é›†ä¸­é—®é¢˜çš„ç›¸ä¼¼åº¦ï¼Œå¯ä»¥ä½¿ç”¨å·²æœ‰æˆ–è€…æ„é€ æ–‡æœ¬ç›¸ä¼¼åº¦æ•°æ®é›†

- è®ºæ–‡/æ¯”èµ›å‘å¸ƒçš„æ•°æ®é›†

ç›¸å…³è®ºæ–‡æ¯”èµ›å‘å¸ƒçš„æ–‡æœ¬ç›¸ä¼¼åº¦æ•°æ®é›†å¯è§ [æ–‡æœ¬ç›¸ä¼¼åº¦æ•°æ®é›†](https://github.com/IceFlameWorm/NLP_Datasets) ï¼Œå¤§éƒ¨åˆ†ä¸ºé‡‘èç­‰ç‰¹å®šé¢†åŸŸæ–‡æœ¬ï¼Œå…¶ä¸­ [LCQMC](http://icrc.hitsz.edu.cn/Article/show/171.html) æä¾›åŸºäºç™¾åº¦çŸ¥é“çš„çº¦ 20w+ å¼€æ”¾åŸŸé—®é¢˜æ•°æ®é›†ï¼Œå¯ä¾›æ¨¡å‹æµ‹è¯•

| data       | total  | positive | negative |
| ---------- | ------ | -------- | -------- |
| training   | 238766 | 138574   | 100192   |
| validation | 8802   | 4402     | 4400     |
| test       | 12500  | 6250     | 6250     |

é™¤æ­¤ä»¥å¤–ï¼Œç™¾åº¦[åƒè¨€é¡¹ç›®](https://www.luge.ai/)å‘å¸ƒäº†[æ–‡æœ¬ç›¸ä¼¼åº¦è¯„æµ‹](https://aistudio.baidu.com/aistudio/competition/detail/45)ï¼ŒåŒ…å« LCQMC/BQ Corpus/PAWS-X ç­‰æ•°æ®é›†ï¼Œå¯ä¾›å‚è€ƒ



- å†…éƒ¨æ•°æ®é›†

å†…éƒ¨ç»™å®šçš„ FAQ æ•°æ®é›†å½¢å¼å¦‚ä¸‹ï¼ŒåŒ…æ‹¬å„ç§â€ä¸»é¢˜/é—®é¢˜â€œï¼Œæ¯ç§â€œä¸»é¢˜/é—®é¢˜â€å¯ä»¥æœ‰å¤šç§ä¸åŒè¡¨è¾¾å½¢å¼çš„é—®é¢˜ `post`ï¼ŒåŒæ—¶å¯¹åº”å¤šç§å½¢å¼çš„å›å¤ `resp` 

æ£€ç´¢æ—¶åªéœ€è¦å°† query ä¸æ‰€æœ‰ post è¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—ï¼Œä»è€Œå¬å›æœ€ç›¸ä¼¼çš„ post ï¼Œç„¶åè·å–å¯¹åº”çš„ â€œä¸»é¢˜/é—®é¢˜â€ çš„æ‰€æœ‰å›å¤ `resp` ï¼Œæœ€åéšæœºè¿”å›ä¸€ä¸ªå›å¤å³å¯

```python
{
   "æ™šå®‰": {
       "post": [
            "10ç‚¹äº†ï¼Œæˆ‘è¦ç¡è§‰äº†",
            "å”‰ï¼Œè¯¥ä¼‘æ¯äº†",
            "å”‰ä¸è·Ÿä½ èŠäº†ç¡è§‰å•¦",
         		...
        ],
       "resp": [
            "ç¥ä½ åšä¸ªå¥½æ¢¦",
            "ç¥ä½ æœ‰ä¸ªå¥½æ¢¦ï¼Œæ™šå®‰ï¼",
            "æ™šå®‰ï¼Œåšä¸ªå¥½æ¢¦ï¼"
            ...
        ]
     },
  	"æ„Ÿè°¢": {
      	"post": [
        		"å¤šè°¢äº†",
       		 "éå¸¸æ„Ÿè°¢",
          ...
      	],
      	"resp": [
        		"åŠ©äººä¸ºä¹ä¸ºå¿«ä¹ä¹‹æœ¬",
        		"åˆ«å®¢æ°”",
          	...
      	]
    },
  	...
}
```



### è´Ÿé‡‡æ ·

å¯¹äºæ¯ä¸ª queryï¼Œéœ€è¦è·å¾—ä¸å…¶ç›¸ä¼¼çš„ **positve candidate** ä»¥åŠä¸ç›¸ä¼¼çš„ **negtive candidate**ï¼Œä»è€Œæ„æˆæ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬ä½œä¸ºæ¨¡å‹è¾“å…¥ï¼Œå³ **(query, candidate)**

âš ï¸ æ­¤å¤„ä¸º **offline negtive sampling**ï¼Œå³åœ¨è®­ç»ƒå‰é‡‡æ ·æ„é€ è´Ÿæ ·æœ¬ï¼ŒåŒºåˆ«äº **online negtive sampling**ï¼Œåè€…åœ¨è®­ç»ƒä¸­çš„æ¯ä¸ª batch å†…è¿›è¡ŒåŠ¨æ€çš„è´Ÿé‡‡æ ·ï¼ˆå¯ä»¥é€šè¿‡ç›¸å…³æŸå¤±å‡½æ•°å®ç°ï¼‰

ä¸¤ç§æ–¹æ³•å¯ä»¥æ ¹æ®ä»»åŠ¡ç‰¹æ€§è¿›è¡Œé€‰æ‹©ï¼Œ**online negtive sampling** å¯¹äºæ•°æ®é›†æœ‰ä¸€å®šçš„è¦æ±‚ï¼Œéœ€è¦ç¡®ä¿æ¯ä¸ª batch å†…çš„ query æ˜¯ä¸ç›¸ä¼¼çš„ï¼Œä½†æ˜¯æ•ˆç‡æ›´é«˜

å¯¹äº **offline negtive sampling** ä¸»è¦ä½¿ç”¨ä»¥ä¸‹ä¸¤ç§æ–¹æ³•ï¼š

- **å…¨å±€è´Ÿé‡‡æ ·**

  åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œæ­£æ€åˆ†å¸ƒé‡‡æ ·ï¼Œå¾ˆéš¾äº§ç”Ÿé«˜éš¾åº¦çš„è´Ÿæ ·æœ¬

- **å±€éƒ¨è´Ÿé‡‡æ ·**

  é¦–å…ˆä½¿ç”¨å°‘é‡äººå·¥æ ‡æ³¨æ•°æ®é¢„è®­ç»ƒçš„ **BERT æ¨¡å‹**å¯¹å€™é€‰é—®é¢˜é›†åˆè¿›è¡Œ**ç¼–ç **ï¼Œç„¶åä½¿ç”¨**æ— ç›‘ç£èšç±»** ï¼Œå¦‚ [Kmeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)ï¼Œæœ€ååœ¨æ¯ä¸ª query æ‰€åœ¨èšç±»ç°‡ä¸­é‡‡æ ·è´Ÿæ ·æœ¬

ä»¥ä¸Šæ‰€æœ‰æ•°æ®ç»¼åˆä½œä¸ºæœ€ç»ˆè®­ç»ƒæ•°æ®

> å¯ä»¥ä½¿ç”¨ **LCQMC** æ ‡æ³¨çš„æ•°æ®è¿›è¡Œ BERT æ¨¡å‹é¢„è®­ç»ƒï¼Œä½†æ˜¯ **LCQMC** æ•°æ®ä¸»è¦ä¸ºç™¾åº¦çŸ¥é“é—®é¢˜ï¼Œä¸FAQä¸­æ—¥å¸¸å¯¹è¯å­˜åœ¨ä¸€å®šå·®å¼‚çš„
>
> [LCQMC: A Large-scale Chinese Question Matching Corpus](https://www.semanticscholar.org/paper/LCQMC%3A-A-Large-scale-Chinese-Question-Matching-Liu-Chen/549c1a581b61f9ea47afc6f6871845392eaebbc4)



### åŒå¡”æ¨¡å‹

å¾®è°ƒè®­ç»ƒæ—¶ä½¿ç”¨**åŸºäºäº¤äº’çš„åŒå¡”æ¨¡å‹** ï¼ˆä¹Ÿç§°ä¸º **Siamese Net** ï¼Œå­ªç”Ÿç½‘ç»œï¼‰ï¼Œä¸è¯­ä¹‰å¬å›æ¨¡å—è·å–å¥å‘é‡çš„å½¢ä¿æŒä¸€è‡´ï¼Œä¸»è¦åŒ…æ‹¬

- **Encoding**ï¼Œä½¿ç”¨ **BERT** åˆ†åˆ«å¯¹ query å’Œ candidate è¿›è¡Œç¼–ç 
- **Pooling**ï¼Œå°†æœ€åä¸€å±‚çš„**å¹³å‡è¯åµŒå…¥ä½œä¸ºå¥å­è¡¨ç¤º**ï¼ˆæˆ–è€… [CLS] çš„å‘é‡è¡¨ç¤ºï¼Œæ•ˆæœå­˜åœ¨å·®åˆ«ï¼‰
- **Computing**ï¼Œè®¡ç®—ä¸¤ä¸ªå‘é‡çš„**ç‚¹ç§¯**ï¼ˆæˆ– **ä½™å¼¦ç›¸ä¼¼åº¦**ï¼‰ä½œä¸ºæœ€ç»ˆå¾—åˆ†

$$
BERT_{rep}(q, d) = dot(\frac{1}{L} \sum_{t=1}^{L}{\vec q_i^{last}}, \frac{1}{L} \sum_{i=1}^{L} {\vec c_i^{last}})
$$

- **åŒå¡”æ¨¡å‹**å¦‚ä¸‹ï¼ˆå…¶ä¸­ $cosine-sim(u,v)$ ä¹Ÿå¯ä»¥æ˜¯å…¶ä»–çš„**åº¦é‡å‡½æ•°**ï¼‰

<img src="https://raw.githubusercontent.com/UKPLab/sentence-transformers/master/docs/img/SBERT_Siamese_Network.png" alt="SBERT Siamese Network Architecture" style="zoom: 50%;" />

> **åŸºäºè¡¨ç¤ºçš„æ¨¡å‹**ï¼Œå³ [BertForSequenceClassification](https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification)ï¼Œå°† query å’Œ cand æ‹¼æ¥ä½œä¸º BERT çš„è¾“å…¥ï¼Œç„¶åå–æœ€åä¸€å±‚ [CLS] å­—æ®µçš„å‘é‡è¡¨ç¤ºè¾“å…¥å…¨è¿æ¥å±‚è¿›è¡Œåˆ†ç±»
> $$
> BERT_{rel}(q, c) = \vec w \times \vec {qd}_{cls}^{last}
> $$
> **åŸºäºè¡¨ç¤ºçš„æ¨¡å‹**æ¯”åŸºäºäº¤äº’çš„æ¨¡å‹æ›´å¥½ï¼Œä½†æ˜¯é€Ÿåº¦è¾ƒæ…¢ï¼Œ ä¸€èˆ¬ç”¨äºæ’åºæ¨¡å—
>
> æ­¤å¤–ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ [poly-encoder](https://arxiv.org/abs/1905.01969) åœ¨åŸºäºè¡¨ç¤ºå’Œäº¤äº’ä¹‹é—´è¿›è¡ŒæŠ˜ä¸­è€ƒè™‘



### æŸå¤±å‡½æ•°

> å‚è€ƒåšå®¢ [Understanding Ranking Loss, Contrastive Loss, Margin Loss, Triplet Loss, Hinge Loss and all those confusing names](https://gombru.github.io/2019/04/03/ranking_loss/) 
>
> åŒæ—¶ç»“åˆ  [SentenceTransformers](https://www.sbert.net/index.html) ä¸­ [Loss API](https://www.sbert.net/docs/package_reference/losses.html) æ€»ç»“

æ¨¡å‹è®­ç»ƒä½¿ç”¨çš„æŸå¤±å‡½æ•°ä¸º **Ranking loss**ï¼Œä¸åŒäºCrossEntropy å’Œ MSE è¿›è¡Œåˆ†ç±»å’Œå›å½’ä»»åŠ¡ï¼Œ**Ranking loss** ç›®çš„æ˜¯é¢„æµ‹è¾“å…¥æ ·æœ¬å¯¹ï¼ˆå³ä¸Šè¿°åŒå¡”æ¨¡å‹ä¸­ $u$ å’Œ $v$ ä¹‹é—´ï¼‰ä¹‹é—´çš„ç›¸å¯¹è·ç¦»ï¼ˆ**åº¦é‡å­¦ä¹ ä»»åŠ¡**ï¼‰



- **Contrastive Loss**

  > æ¥è‡ª LeCun [Dimensionality Reduction by Learning an Invariant Mapping](http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf)
  >
  > [sentence-transformers æºç ](https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/ContrastiveLoss.py)

  - å…¬å¼å½¢å¼å¦‚ä¸‹ï¼Œå…¶ä¸­ $u, v$ ä¸º BERT ç¼–ç çš„å‘é‡è¡¨ç¤ºï¼Œ$y$ ä¸ºå¯¹åº”çš„æ ‡ç­¾ï¼ˆ1 è¡¨ç¤ºæ­£æ ·æœ¬ï¼Œ0 è¡¨ç¤ºè´Ÿæ ·æœ¬ï¼‰ï¼Œ $\tau$ ä¸ºè¶…å‚æ•°
    $$
    L(u_i, v_i, y_i) =  y_i ||u_i, v_i|| + (1 - y_i) \max(0, \tau - ||u_i, v_i||
    $$

  - å…¬å¼æ„ä¹‰ä¸ºï¼šå¯¹äº**æ­£æ ·æœ¬**ï¼Œè¾“å‡ºç‰¹å¾å‘é‡ä¹‹é—´è·ç¦»è¦**å°½é‡å°**ï¼›è€Œå¯¹äº**è´Ÿæ ·æœ¬**ï¼Œè¾“å‡ºç‰¹å¾å‘é‡é—´è·ç¦»è¦**å°½é‡å¤§**ï¼›ä½†æ˜¯è‹¥**è´Ÿæ ·æœ¬é—´è·å¤ªå¤§**ï¼ˆå³å®¹æ˜“åŒºåˆ†çš„**ç®€å•è´Ÿæ ·æœ¬**ï¼Œé—´è·å¤§äº $\tau$ï¼‰**åˆ™ä¸å¤„ç†**ï¼Œè®©æ¨¡å‹å…³æ³¨æ›´åŠ **éš¾ä»¥åŒºåˆ†**çš„æ ·æœ¬



- **OnlineContrastive Loss**

  - å±äº **online negative sampling** ï¼Œä¸ **Contrastive Loss** ç±»ä¼¼

  - å‚è€ƒ [sentence-transformers æºç ](https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/OnlineContrastiveLoss.py) ï¼Œåœ¨æ¯ä¸ª batch å†…ï¼Œé€‰æ‹©æœ€éš¾ä»¥åŒºåˆ†çš„æ­£ä¾‹æ ·æœ¬å’Œè´Ÿä¾‹æ ·æœ¬è¿›è¡Œ loss è®¡ç®—ï¼ˆå®¹æ˜“è¯†åˆ«çš„æ­£ä¾‹å’Œè´Ÿä¾‹æ ·æœ¬åˆ™å¿½ç•¥ï¼‰

  - å…¬å¼å½¢å¼å¦‚ä¸‹ï¼Œå¦‚æœæ­£æ ·æœ¬è·ç¦»å°äº $\tau_1$ åˆ™ä¸å¤„ç†ï¼Œå¦‚æœè´Ÿæ ·æœ¬è·ç¦»å¤§äº $\tau_0$ åˆ™ä¸å¤„ç†ï¼Œå®ç°è¿‡ç¨‹ä¸­ $\tau_0, \tau_1$ å¯ä»¥åˆ†åˆ«å–è´Ÿ/æ­£æ ·æœ¬çš„å¹³å‡è·ç¦»å€¼
    $$
    L(u_i, v_i, y_i) 
    \begin{cases}
    \max (0, ||u_i, v_i|| - \tau_1) ,\ if \ y_i=1 & \\
    \max (0, \tau_0 - ||u_i, v_i||),\ if \ y_i =0
    \end{cases}
    $$



- **Multiple Negatives Ranking Loss**

  > æ¥è‡ªGoogle [Efficient Natural Language Response Suggestion for Smart Reply](https://arxiv.org/pdf/1705.00652.pdf)
  >
  > [sentence-transformers æºç ](https://github.com/UKPLab/sentence-transformers/blob/master/sentence_transformers/losses/MultipleNegativesRankingLoss.py)

  - å±äº **online negative sampling** ï¼Œå½“å¦‚æœåªæœ‰æ­£æ ·æœ¬ï¼ˆä¾‹å¦‚åªæœ‰ç›¸ä¼¼çš„æ®µè½/é—®ç­”å¯¹ç­‰ï¼‰æ—¶ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­åœ¨æ¯ä¸ª batch å†…è¿›è¡Œè´Ÿé‡‡æ ·
  - è¾“å…¥å¥å­å¯¹ $(a_1, b_1), (a_2, b_), \cdots, (a_n, b_n)$ ï¼Œå‡è®¾ $(a_i, b_i)$ æ˜¯æ­£æ ·æœ¬ï¼Œ$(a_i, b_j), i \ne j$  æ˜¯è´Ÿæ ·æœ¬
  - åœ¨æ¯ä¸ª batch å†…ï¼Œå¯¹äº $a_i$ ï¼Œå…¶ä½¿ç”¨æ‰€æœ‰å…¶ä»–çš„ $b_j$ ä½œä¸ºè´Ÿæ ·æœ¬ï¼Œç„¶åä¼˜åŒ– negative log-likehood loss
  - ä¾èµ–äºè´Ÿæ ·æœ¬ä¹‹é—´çš„è·ç¦»ï¼Œå¦‚æœè·ç¦»å·®è·å°ï¼Œåˆ™æ•ˆæœå¯èƒ½ä¸å¥½



- **Cosine Similarity Loss**

  - å¯¹äºæ¯ä¸ªæ ·æœ¬çš„ query å’Œ candidate ä¹‹é—´è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œç„¶åä¸æ ‡å‡†ç›¸ä¼¼åº¦ä¹‹é—´è®¡ç®— MSE æŸå¤±

  - å¯¹äºè¾“å…¥çš„æ ‡æ³¨ç›¸ä¼¼åº¦ï¼ˆå³ labelï¼‰éœ€è¦æ˜¯ float ç±»å‹ï¼ˆå¦‚æœæ ‡ç­¾ä¸º 0 æˆ– 1ï¼Œæ‹Ÿåˆæ•ˆæœå¯èƒ½æ¬ ä½³ï¼‰

  - å…¬å¼å½¢å¼å¦‚ä¸‹
    $$
    L(u, v, y) = (\frac{u \cdot v}{||u||_2 \times ||v||_2} - y)^2
    $$



- **Pairwise Max Margin Hinge loss**

  > çŸ¥ä¹æœç´¢ç³»ç»Ÿè®ºæ–‡ä¸­ç”¨åˆ°çš„æŸå¤±å‡½æ•°
  >
  > [Beyond Lexical: A Semantic Retrieval Framework for Textual SearchEngine](http://arxiv.org/abs/2008.03917)

  - è¿ç”¨ SVM ä¸­ max margin æ€æƒ³ï¼Œå°½é‡æ‹‰è¿œæ­£è´Ÿæ ·æœ¬ä¹‹é—´çš„é—´è·
  - è®­ç»ƒè¿‡ç¨‹ä¸­åœ¨æ¯ä¸ª batch å†…ï¼Œåº¦é‡ä¸¤ä¸ªæ­£è´Ÿæ ·æœ¬ï¼ˆpairwiseï¼‰ä¹‹é—´çš„è·ç¦»
  - å…¬å¼å¦‚ä¸‹ï¼Œå…¶ä¸­ $p_i$ å’Œ $p_j$ åˆ†åˆ«è¡¨ç¤ºæ¯ä¸ª $<query, candidate>$ çš„æ¨¡å‹è®¡ç®—å¾—åˆ†ï¼Œ$y_i$ å’Œ $y_j$ è¡¨ç¤ºæ¯ä¸ªå€™é€‰é—®é¢˜çš„æ ‡ç­¾ï¼Œ$\tau$ ä¸ºç”¨äºå¯¹é½çš„è¶…å‚æ•°

  $$
  \frac{1}{M} \sum_i \sum_{j \ne i} \max (0, \tau - (y_i - y_j) \times (p_i -p_j))
  $$



### fine-tune

fine-tune å¯ä»¥ä½¿ç”¨é«˜åº¦å°è£…çš„ [Sentence-Transformers](https://www.sbert.net/index.html)  å¿«é€Ÿæ­å»ºæ¨¡å‹ï¼Œæˆ–è€…ä½¿ç”¨ [Transformers](https://huggingface.co/transformers/) æ›´çµæ´»åœ°å®ç°è‡ªå®šä¹‰æ¨¡å‹

| æ¡†æ¶                                                      | ç‰¹ç‚¹                                                         |
| --------------------------------------------------------- | ------------------------------------------------------------ |
| [Sentence-Transformers](https://www.sbert.net/index.html) | ç®€å•æ˜“ç”¨<br />å†…ç½®å¤šç§ Ranking loss<br />æ’ä»¶å¼æ¨¡å—ä½¿ç”¨<br />ä¸»è¦ç”¨äºå¥å¯¹åˆ†ç±»ä»»åŠ¡<br />ä¸æ”¯æŒå¤šGPUè®­ç»ƒ |
| [Transformers](https://huggingface.co/transformers/)      | å†…ç½®å¤šç§è®­ç»ƒæœºåˆ¶<br />åŒ…æ‹¬åŠç²¾åº¦ã€åˆ†å¸ƒå¼è®­ç»ƒç­‰<br />è‡ªå®šä¹‰ç¨‹åº¦è¾ƒé«˜ï¼Œé€‚åˆå„ç§ä»»åŠ¡ |

ä»¥ä¸‹åˆ†åˆ«ä»‹ç»ä»£ç å®ç°æ­¥éª¤

- [Sentence-Transformers](https://www.sbert.net/index.html) 

  - å¯ä»¥è¿›è¡Œå¤šç§ä»»åŠ¡çš„ fine-tuneï¼Œè¿™é‡Œå¯ä»¥é€‰æ‹©ç±»ä¼¼çš„ [Quora Duplicate Questions](https://www.sbert.net/examples/training/quora_duplicate_questions/README.html) ä½œä¸ºå‚è€ƒï¼Œå³åˆ¤æ–­ä¸¤ä¸ªé—®é¢˜æ˜¯å¦ç›¸ä¼¼ï¼Œæ ‡ç­¾ä¸º 0 å’Œ 1

  - é¦–å…ˆéœ€è¦åˆå§‹åŒ–é¢„è®­ç»ƒæ¨¡å‹ï¼Œ`model_name` å¿…é¡»æ˜¯ Sentece-Transformers [å†…ç½®çš„é¢„è®­ç»ƒæ¨¡å‹](https://public.ukp.informatik.tu-darmstadt.de/reimers/sentence-transformers/v0.2/) ï¼Œä¹Ÿå¯ä»¥åŠ è½½ transformers çš„æ¨¡å‹ï¼ˆä¸å‰æ–‡ä¸€è‡´ï¼‰

    ```python
    from sentence_transformers import SentenceTransformer, models
    # åŠ è½½å†…ç½®é¢„è®­ç»ƒæ¨¡å‹
    model = SentenceTransformer('model_name')
    
    # å¯ä»¥è‡ªå®šä¹‰æ¨¡å— æˆ– ä½¿ç”¨ HugginFace Transformers é¢„è®­ç»ƒæ¨¡å‹
    #word_embedding_model = models.Transformer('bert-base-chinese', max_seq_length=256)
    # å¯ä»¥é€‰æ‹©ä½¿ç”¨ cls/max/mean ç­‰å¤šç§ pooling æ–¹å¼
    #pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())
    #model = SentenceTransformer(modules=[word_embedding_model, pooling_model])
    ```

  - ä¹‹ååˆ›å»ºè®­ç»ƒé›†ï¼Œé€šè¿‡å†…ç½®çš„ InputExample æ„å»ºæ ·æœ¬

    ```python
    from torch.utils.data import DataLoader
    from sentence_transformers import SentencesDataset
    from sentence_transformers.readers import InputExample
    
    # æ¯ä¸ªæ ·æœ¬ä½¿ç”¨ InputExample è¿›è¡Œå°è£…
    train_samples = [InputExample(texts=['sentence1', 'sentence2'], label=1)]
    train_dataset = SentencesDataset(train_samples, model=model)
    train_dataloader = DataLoader(train_dataset,
                                  shuffle=True,
                                  batch_size=64)
    # ä½¿ç”¨ä½™å¼¦è·ç¦»ä½œä¸ºåº¦é‡æŒ‡æ ‡ (cosine_distance = 1-cosine_similarity)
    distance_metric = losses.SiameseDistanceMetric.COSINE_DISTANCE
    # åˆå§‹åŒ–è®­ç»ƒæŸå¤±å‡½æ•°
    train_loss = losses.OnlineContrastiveLoss(model=model,
                                              distance_metric=distance_metric,
                                              margin=0.5)
    ```

  - ç„¶åå¯ä»¥é’ˆå¯¹å…·ä½“ä»»åŠ¡è¯„ä¼°å™¨

    - ç®€å•çš„åˆ†ç±»ï¼Œç»™å®š (sentence1, sentence2) é€šè¿‡è®¡ç®—ä¸¤ä¸ªå¥å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦åˆ¤æ–­æ˜¯å¦ç›¸ä¼¼ï¼Œå¦‚æœé«˜äºæŸä¸ªé˜ˆå€¼åˆ™åˆ¤æ–­ä¸ºç›¸ä¼¼ï¼ˆæºç å¯¹å¼€å‘é›†ç»“æœè¿›è¡Œæ’åºï¼Œä¾æ¬¡å–ä¸¤ä¸ªç›¸é‚»å¾—åˆ†ç»“æœä½œä¸ºé˜ˆå€¼ï¼Œéå†æŸ¥æ‰¾æœ€é«˜çš„ f1 å€¼ä½œä¸ºç»“æœï¼‰

    ```python
    from sentence_transformers import evaluation
    from sentence_transformer import losses
    
    dev_sents1, dev_sents2, dev_labels = ['sent1', 'sent2'], ['sent11', 'sent22'], [1, 0]
    binary_acc_evaluator = evaluation.BinaryClassificationEvaluator(
            dev_sentences1, dev_sentences2, dev_labels)
    ```

    - è¿˜å¯ä»¥æ„å»ºå¤šç§è¯„ä¼°å™¨ï¼Œä½¿ç”¨ `evaluation.SequentialEvaluator()` ä¾æ¬¡è¿›è¡Œè¯„ä¼°ï¼Œå‚è€ƒ [training_OnlineConstrativeLoss.py ](https://github.com/UKPLab/sentence-transformers/blob/master/examples/training/quora_duplicate_questions/training_OnlineConstrativeLoss.py) åŒæ—¶è¿›è¡Œåˆ†ç±»ä»¥åŠæ£€ç´¢ç­‰è¯„ä¼°

  - æœ€åè¿›è¡Œæ¨¡å‹è®­ç»ƒ

    ```python
    model.fit(train_objectives=[(train_dataloader, train_loss)],
              evaluator=binary_acc_evaluator,
              epochs=10,
              warmup_steps=1000,
              output_path='output',
              output_path_ignore_not_empty=True)
    ```

> é«˜åº¦å°è£…ï¼Œè®­ç»ƒæµç¨‹å’Œ sklearn ä»¥åŠ keras ç±»ä¼¼ï¼Œç®€å•å®¹æ˜“ç†è§£
>
> ä¸æ”¯æŒå¤šGPUè®­ç»ƒï¼Œ[Multi-GPU-training #311](https://github.com/UKPLab/sentence-transformers/issues/311#issuecomment-659455875)
>
> æ›´å¤šä¾‹å­å‚è€ƒ [sentence-transformers/examples](https://github.com/UKPLab/sentence-transformers/tree/master/examples)



- [Transformers](https://huggingface.co/transformers/)

  - é™¤äº†ä½¿ç”¨ [SentenceTransformers](https://www.sbert.net/index.html) ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ä½¿ç”¨ [Transformers](https://huggingface.co/transformers/) çš„ [Trainer](https://huggingface.co/transformers/main_classes/trainer.html) æ¥å£è¿›è¡Œå¿«é€Ÿæ¨¡å‹è®­ç»ƒ
  - [Trainer](https://huggingface.co/transformers/main_classes/trainer.html) åŸºæœ¬ç”¨æ³•å‚è€ƒ [Training and fine-tuning](https://huggingface.co/transformers/training.html#trainer) ï¼Œå†…éƒ¨å°è£… PyTorch è®­ç»ƒ/é¢„æµ‹åŸºæœ¬æµç¨‹ï¼Œåªéœ€è¦ç¡®å®š model å’Œ Dataset å³å¯
  - ä¸ºäº†å®ç°åŸºäº BERT çš„ SiameseNetworkï¼Œéœ€è¦è‡ªå®šä¹‰ Model ä»¥åŠ Datasetï¼Œå…·ä½“æ­¥éª¤å¦‚ä¸‹
    - è¯»å–æ‰€æœ‰å¥å¯¹æ ·æœ¬ä¿å­˜ `List[InputExample]` ï¼Œ`InputExample` åŒ…å«å¥å¯¹ä¿¡æ¯
    - åˆå§‹åŒ– `SiameseDataset(examples, tokenizer)` ï¼Œå®ç° `__getitem__` æ–¹æ³•ï¼ˆè¿›è¡Œåˆ†è¯ç­‰æ“ä½œï¼Œè¿”å›ä¸º token id åºåˆ—ï¼‰
    - å®ç° `collate_fn` å‡½æ•°ï¼Œç”¨äº `DataLoader` å›è°ƒï¼Œå¯¹äºæ¯ä¸ª batch çš„æ•°æ®è¿›è¡Œå¤„ç†ï¼ˆåˆ†åˆ«å¯¹ batch å†…æ‰€æœ‰ sentence1 å’Œ sentence2 è¿›è¡Œè¡¥é½ï¼Œ å¹¶å‡†å¤‡ `BertForSiameseNetwork` æ¨¡å‹è¾“å…¥çš„ç‰¹å¾ï¼‰
    - å®ç° `BertForSiameseNetwork` ï¼Œç»§æ‰¿ `BertPreTrainedModel` ï¼Œä½¿ç”¨ `BertModel` ä½œä¸º encoderï¼›å®ç° `forward` å‡½æ•°ï¼Œè‡ªå®šä¹‰è¾“å…¥ç‰¹å¾å‚æ•°ï¼Œä½¿ç”¨ encoder åˆ†åˆ«ç¼–ç è·å¾— sentence1å’Œ sentence2 çš„å¥å‘é‡ï¼Œè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ä½œä¸º loss è¿”å›
    - å®ç° `compute_metrics` ï¼Œè¿›è¡ŒæŒ‡æ ‡è®¡ç®—
    - åˆå§‹åŒ– `Trainer` å®ä¾‹ï¼Œä¼ å…¥ modelï¼Œdatasetï¼Œcollate_fn ä»¥åŠ compute_metrics




### æ¨¡å‹è’¸é¦

 [TextBrewer](https://github.com/airaria/TextBrewer) åŸºäº Transformers çš„æ¨¡å‹è’¸é¦å·¥å…·ï¼Œ[å®˜æ–¹ å…¥é—¨ç¤ºä¾‹](https://github.com/airaria/TextBrewer/tree/master/examples/random_tokens_example) ï¼Œ[å®˜æ–¹ cmrc2018ç¤ºä¾‹](https://github.com/airaria/TextBrewer/tree/master/examples/cmrc2018_example)

ä½¿ç”¨ä¸Šè¿° transformers Trainer ä¸­å®ç°çš„ `BertForSiameseNet` ï¼Œ`SiameseDataset` ç­‰è‡ªå®šä¹‰ç±»

ä¸»è¦æ­¥éª¤ï¼š

- è¯»å– teacher å’Œ student çš„é…ç½®æ–‡ä»¶ï¼ˆstudent é…ç½®æ–‡ä»¶å¯ä»¥ç›´æ¥ä¿®æ”¹ teacher ä¸­çš„ `num_hidden_layer`ï¼‰

- æ„å»º `SiameseDataset` æ•°æ®é›†ï¼Œè°ƒç”¨ `Collator.batching_collate` å‡½æ•°å¹¶åˆå§‹åŒ– `DataLoader` ï¼ˆ`Trainer` ä¸­å†…éƒ¨å®Œæˆåˆå§‹åŒ–ï¼Œ`TextBrewer` éœ€è¦ç›´æ¥ä¼ å…¥ï¼‰

- åˆ›å»º teacher å’Œ student çš„ `BertForSiameseNet` æ¨¡å‹ï¼Œå¹¶åŠ è½½é¢„è®­ç»ƒå‚æ•°ï¼ˆteacher æ¨¡å‹è®¾ç½®ä¸º `eval` æ¨¡å¼ï¼Œ å³è®­ç»ƒè¿‡ç¨‹ä¸­å‚æ•°ä¸æ”¹å˜ï¼Œ åªæä¾›ä¸­é—´å±‚å’Œè¾“å‡ºå±‚çŠ¶æ€ç»™ student å‚è€ƒ ï¼‰

- åˆ›å»º optimizer ä»¥åŠ TrainingConfig å’Œ DistillationConfig ï¼ˆä»¥åŠ `intermediate_matches` ï¼Œå³ teacher å’Œ student ä¸­é—´å±‚çš„æ˜ å°„æ–¹æ³•ï¼‰

- åˆ›å»º `Adaptor`ï¼Œç”¨äºè§£é‡Š `BertForSiamesNet` çš„è¾“å‡ºï¼Œç”¨äºåç»­æ˜ å°„è®¡ç®—

  > æ ¹æ® `intermediate_matches` ï¼Œæ¨¡å‹éœ€è¦è¾“å‡º hidden_states
  >
  > ç„¶å `BertForSiameseNet` ä¸­æ¯ä¸ªå¥å­éƒ½æœ‰ hidden_statesï¼Œç›®å‰è§£å†³æ–¹æ³•æ—¶ç›´æ¥é€‰æ‹©ç¬¬ä¸€ä¸ªå¥å­çš„è¿”å›

- åˆ›å»º `GeneralDistiller`

- åˆ›å»ºè¯„ä¼°å›è°ƒå‡½æ•° `predict` ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ä¹‹å‰çš„ `compute_metrics` è®¡ç®—å„æŒ‡æ ‡



### å®éªŒç»“æœ

- [ ] åŸºäº bert-base-chinese è·‘ hflqa(samples/train/test.csv) æ•°æ®ï¼Œ:running:
- [ ] åŸºäº bert-base-chinese è·‘ hflqa+lcqmc (samples/merge_train/test.csv) æ•°æ®ï¼Œ:running:
- [ ] è’¸é¦ä¸Šè¿°æ¨¡å‹åˆ° 6å±‚ bertï¼Œ+ ddqa.csv
- [ ] ç›´æ¥ä½¿ç”¨ 6å±‚ bert + ä¸Šè¿°æ•°æ®

- [ ] 
- [ ] è·å¾—é€šç”¨é¢†åŸŸ 6-layers BERT å¥å‘é‡ç¼–ç å™¨

| model/pretrained<br/>dataset/layers                          | lcqmc<br/>acc | lcqmc<br/>f1  | hflqa<br/>acc | hflqa<br/>f1 | hflqa<br/>hit@1   | ddqa<br/>hit@1    |
| ------------------------------------------------------------ | ------------- | ------------- | :------------ | ------------ | ----------------- | ----------------- |
| *bert-base-chinese*                                          | -             | -             | -             | -            | 0.7394            | -                 |
| *6 layers from above* :point_up_2:                           | -             | -             | -             | -            | 0.7276            | -                 |
| *6 layers from above* :point_up_2:<br/>:steam_locomotive: â€‹merge dataset | 0.8789/0.8664 | 0.8799/0.8660 | 0.9100        | 0.8920       | 0.8463            | -                 |
| sentence-transformers<br/>(bert-base-chinese)<br/>:steam_locomotive: â€‹lcqmc dataset | 0.8890/0.8796 | 0.8898/0.8794 | 0.7333        | 0.5212       | 0.8350            | -                 |
| *model from above* :point_up_2: <br/>:steam_locomotive: â€‹hflqa data | -             | -             | -             | -            | 0.8562            | -                 |
| ğŸ¤— transformers<br />BertForSiameseNet<br/>(bert-base-chinese)<br/>:steam_locomotive: â€‹lcqmc dataset | 0.8818/0.8705 | 0.8810/0.8701 | 0.7333        | 0.5212       | 0.8177            | -                 |
| *model from above* :point_up_2: <br />:steam_locomotive: merge dataset | 0.8848/0.8751 | 0.8837/0.8749 | 0.9225        | 0.9053       | 0.8567            | 0.8500            |
| *6 layers from above* :point_up_2:                           | 0.6791/0.7417 | 0.6584/0.7403 | 0.7913        | 0.7429       | 0.7975            |                   |
| ğŸ¤— transformers<br/>BertForSeqClassify<br/> (bert-base-chinese)<br/>:steam_locomotive: â€‹lcqmc dataset | 0.8832/0.8600 | 0.8848/0.8706 | -             | -            | -                 | -                 |
| ğŸ¤— transformers + textbrewer<br/>BertForSiameseNet<br/>distillation-L6<br/>:steam_locomotive: â€‹merge dataset + ddqa |               |               |               |              |                   |                   |
| ğŸ¤— transformers<br />BertForSiameseNet<br/>:steam_locomotive: merge3(l<br/>cqmc+hflqa+ddqa) |               |               |               |              |                   |                   |
| checkpoint-3000<br/>12 vs 6-layers:point_up_2:               | -             | -             | -             | -            | 0.8980<br/>0.9128 | 0.9961<br/>0.8201 |

> å†…éƒ¨ hflqa æ•°æ®é›†ï¼ˆpostsï¼‰æŒ‰9:1æ‹†åˆ†ï¼Œacc å’Œ f1 ä¸ºé—®é¢˜å¯¹å½¢å¼æµ‹è¯•é›†ï¼ˆæ ¹æ®é˜ˆå€¼åˆ†ç±»ï¼‰ï¼Œhit@1 ä¸ºæ£€ç´¢æµ‹è¯•é›†ï¼ˆçœŸå®åœºæ™¯ï¼‰
>
> | dataset     | topics | posts | positive(sampling) | negative(sampling) | total(sampling) |
> | ----------- | ------ | ----- | ------------------ | ------------------ | --------------- |
> | hflqa-train | 1468   | 18267 | 5w+                | 5w+                | 10w+            |
> | hflqa-test  | 768    | 2030  | 2984               | 7148               | 10132           |
> | hflqa-total | 1500   | 20297 | -                  | -                  | -               |
> | lcqmc       | -      | -     | -                  | -                  | 24w+            |
> | ddqa        | -      | 12w+  | 50w+               | 50w+               | 100w+           |
> | merge3      | -      | -     | -                  | -                  | 134w+           |

å…³äºé‡‡æ ·çš„å‡ ä¸ªç»“æœ

- èšç±»ç»“æœè¶Šå°‘ï¼Œè´Ÿä¾‹è¶Šç®€å•ï¼Œå¯ä»¥ç”¨åŸå§‹ä¸»é¢˜æ•°/ç±»åˆ«æ•° é™¤ä»¥5
- gmm å’Œ kmeans å·®åˆ«ä¸æ˜¯å¾ˆå¤§

å…³äº hflqa å¬å›ç»“æœ

- hflqa + lcqmc
  - 2000 hflqa æµ‹è¯•é›† hit@1 å¤§çº¦ 85% å·¦å³
  - é”™è¯¯åŸå› ä¸»è¦æ˜¯ hflqa æ•°æ®é—®é¢˜
    - å­˜åœ¨æ„æ€ç›¸åŒçš„ topicï¼Œè‡ªåŠ¨è¯„æµ‹æ— æ³•åŒºåˆ†ï¼Œè®¤å®šä¸ºå¤±è´¥ï¼ˆå®é™…æ˜¯æ­£ç¡®çš„ï¼‰
    - ä¸€äº›ä¸å¸¸ç”¨è¡¨è¾¾æˆ–è€…è¡¨è¾¾ä¸å®Œæ•´çš„å¥å­
    - æ­£å¸¸å¯¹è¯çš„å¬å›ç‡è¾ƒé«˜

- hflqa + lcqmc + ddqa
  - 2000 hflqa æµ‹è¯•é›†ï¼Œ6å±‚æ¯”12å±‚æ•ˆæœå¥½ä¸€ä¸ªç‚¹ï¼Œhit@1 çº¦ 90%
  - 10000 ddqa æµ‹è¯•é›†ï¼Œ12å±‚ hit@1 è¾¾åˆ° 99%ï¼Œ6å±‚åªæœ‰ 82%
  - å‰é¢çš„å±‚å­¦åˆ°äº†è¾ƒä¸º



## FAQ WebæœåŠ¡æ­å»º

### Flask æ­å»ºWebAPI

- Flask + Gunicorn + gevent + nginx ï¼Œè¿›ç¨‹ç®¡ç†ï¼ˆè‡ªå¯åŠ¨ï¼‰ï¼ˆuwsgi åŒç†ï¼Œgunicorn æ›´ç®€å•ï¼‰
-  [werkzeug.contrib.cache](https://werkzeug.palletsprojects.com/en/0.16.x/contrib/cache/)  ä½¿ç”¨ cache ç¼“å­˜é—®é¢˜ï¼Œ æ³¨æ„ç‰ˆæœ¬ werkzeug==0.16.0



### Locust å‹åŠ›æµ‹è¯•

ä½¿ç”¨ [Locust](https://locust.io/) ç¼–å†™å‹åŠ›æµ‹è¯•è„šæœ¬



## æ›´å¤š

- æœ¬æ–‡é€‚ç”¨äºå°è§„æ¨¡æ•°æ®ï¼ˆå°äº10wï¼Ÿï¼‰ï¼Œå¯¹äºæ›´å¤§è§„æ¨¡çš„æ£€ç´¢å¯ä»¥å‚è€ƒ [facebookresearch/faiss](3https://github.com/facebookresearch/faiss) ï¼ˆå»ºç«‹å‘é‡ç´¢å¼•ï¼Œç„¶åä½¿ç”¨ k-nearest- neighbor è¿›è¡Œå¬å›ï¼‰

- å¾ˆå¤šåœºæ™¯ä¸‹ï¼ŒåŸºäºå…³é”®å­—çš„å€’æ’ç´¢å¼•å¬å›ç»“æœå·²ç»è¶³å¤Ÿï¼Œå¯ä»¥è€ƒè™‘ç»¼åˆåŸºäºå…³é”®å­—å’ŒåŸºäºå‘é‡çš„å¬å›æ–¹æ³•ï¼Œå‚è€ƒ [Query ç†è§£å’Œè¯­ä¹‰å¬å›åœ¨çŸ¥ä¹æœç´¢ä¸­çš„åº”ç”¨](https://www.6aiq.com/article/1577969687897)



